{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMa9semeWz/SZgqQnBu6/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Ann/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "7_qKd9be33sh"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#دانلود دیتاست\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
        "download_url(dataset_url, '.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1FRnxdM33qE",
        "outputId": "bffb326f-f3e7-43af-a6e8-d1c4bd856fd4"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./cifar10.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')\n",
        "     "
      ],
      "metadata": {
        "id": "hevRD3J733nb"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './data/cifar10'\n",
        "\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwpOljA333bn",
        "outputId": "39df57f8-7c54-49d6-8eca-56f8f4d8cb43"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'train']\n",
            "['dog', 'cat', 'frog', 'bird', 'truck', 'horse', 'deer', 'ship', 'automobile', 'airplane']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "3Sc4GnXh8rQH"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
      ],
      "metadata": {
        "id": "QxqQA5kc8rcV"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed);"
      ],
      "metadata": {
        "id": "GZ1WUxuj8zMm"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "batch_size=64"
      ],
      "metadata": {
        "id": "xjcrKHviIuP5"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_size = 5000\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZEQbMt98zPj",
        "outputId": "822a26fb-4968-4706-dc6c-68ec9f52d3a6"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "metadata": {
        "id": "lrhjXb5v8zS0"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "batch_size=64"
      ],
      "metadata": {
        "id": "b-mk0AQZFkxe"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SCUeIJqFsox",
        "outputId": "18e5804f-471f-4264-af93-c860de31e95b"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dl = DataLoader(train_ds)\n",
        "# val_dl = DataLoader(val_ds)\n",
        "     "
      ],
      "metadata": {
        "id": "jyGsOYT6OUPS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_default_device():\n",
        "#     \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "#     if torch.cuda.is_available():\n",
        "#         return torch.device('cuda')\n",
        "#     else:\n",
        "#         return torch.device('cpu')\n",
        "    \n",
        "# def to_device(data, device):\n",
        "#     \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "#     if isinstance(data, (list,tuple)):\n",
        "#         return [to_device(x, device) for x in data]\n",
        "#     return data.to(device, non_blocking=True)\n",
        "\n",
        "# class DeviceDataLoader():\n",
        "#     \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "#     def __init__(self, dl, device):\n",
        "#         self.dl = dl\n",
        "#         self.device = device\n",
        "        \n",
        "#     def __iter__(self):\n",
        "#         \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "#         for b in self.dl: \n",
        "#             yield to_device(b, self.device)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         \"\"\"Number of batches\"\"\"\n",
        "#         return len(self.dl)"
      ],
      "metadata": {
        "id": "MHlh32hUOkfU"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = get_default_device()\n",
        "# device"
      ],
      "metadata": {
        "id": "opHuU6q1Om8D"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "VwFEIOjsJL0m"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dl = DeviceDataLoader(train_dl, device)\n",
        "# val_dl = DeviceDataLoader(val_dl, device)\n",
        "     "
      ],
      "metadata": {
        "id": "0tgdD8wA8zVq"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "coDoheZo8zYH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CSLl0XiGOwI-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-OTwEDLOwLm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pm50pPCh8zar"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transition Layers"
      ],
      "metadata": {
        "id": "t1YMuadT30dR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "rb0fib5z3yb2"
      },
      "outputs": [],
      "source": [
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseLayer Implementation"
      ],
      "metadata": {
        "id": "8yS7N9tP4yxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need is to implement the dense layer inside a dense block."
      ],
      "metadata": {
        "id": "6E7UPjxZ40jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
        "                                           growth_rate, kernel_size=1, stride=1,\n",
        "                                           bias=False)),\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1,\n",
        "                                           bias=False)),\n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs):\n",
        "        \"Bottleneck function\"\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
        "        return bottleneck_output\n",
        "\n",
        "    def forward(self, input):  # noqa: F811\n",
        "        if isinstance(input, torch.Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        bottleneck_output = self.bn_function(prev_features)\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
        "                                     training=self.training)\n",
        "        return new_features"
      ],
      "metadata": {
        "id": "v4gZDcJo3-Bb"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseBlock Implementation\n",
        "Now, we are ready to implement the DenseBlock which consists of multiple such DenseLayers."
      ],
      "metadata": {
        "id": "d1D-ep4L5h9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features):\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)"
      ],
      "metadata": {
        "id": "tciRpmP2456u"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNet Architecture Implementation\n",
        "Finally, we are now ready to implement the DenseNet architecture as we have already implemented the DenseLayer and DenseBlock."
      ],
      "metadata": {
        "id": "QrAQ8eiM56Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "TzRODRzytEKM"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=10, memory_efficient=False):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # Convolution and pooling part from table-1\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
        "                                padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "        # Add multiple denseblocks based on config \n",
        "        # for densenet-121 config: [6,12,24,16]\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient\n",
        "            )\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                # add transition layer between denseblocks to \n",
        "                # downsample\n",
        "                trans = _Transition(num_input_features=num_features,\n",
        "                                    num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1uFXa7F55izY"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use the above implementation to create densenet-121 architecture.\n",
        "\n"
      ],
      "metadata": {
        "id": "FCTXWojM6LLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n",
        "              **kwargs):\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "    return model\n",
        "\n",
        "def densenet121(pretrained=False, progress=True, **kwargs):\n",
        "    return _densenet('densenet121', 32, (6, 12, 24, 16), 32, pretrained, progress,\n",
        "                     **kwargs)"
      ],
      "metadata": {
        "id": "pGOSktGq6HLU"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def densenet161(pretrained=False, progress=True, **kwargs):\n",
        "  return _densenet('densenet161', 48, (6, 12, 36, 24), 96, pretrained, progress,\n",
        "                     **kwargs)"
      ],
      "metadata": {
        "id": "2wOILpE16OPm"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def densenet169(pretrained=False, progress=True, **kwargs):\n",
        "  return _densenet('densenet169', 32, (6, 12, 32, 32), 64, pretrained, progress,\n",
        "                     **kwargs)"
      ],
      "metadata": {
        "id": "Ogcfg9jc60yO"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def densenet201(pretrained=False, progress=True, **kwargs):\n",
        "   return _densenet('densenet201', 32, (6, 12, 48, 32), 64, pretrained, progress,\n",
        "                     **kwargs)"
      ],
      "metadata": {
        "id": "HHW06ox1SHyB"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apt get models"
      ],
      "metadata": {
        "id": "6kASgjMTXNyS"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install utils"
      ],
      "metadata": {
        "id": "b2qqlnJ2dkPC"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install progress_bar"
      ],
      "metadata": {
        "id": "yUR9M86fjgPr"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import progress_bar"
      ],
      "metadata": {
        "id": "5abqPaINjj61"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install models"
      ],
      "metadata": {
        "id": "CvO2dXr1k15W"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "mbuqwqABp8ib"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from __future__ import print_function\n",
        "\n",
        "# import argparse\n",
        "# import os\n",
        "\n",
        "# import numpy\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.optim import lr_scheduler\n",
        "# import torch.backends.cudnn as cudnn\n",
        "\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# # from models import PreActResNet18\n",
        "# # from utils import progress_bar\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true',\n",
        "#                     help='resume from checkpoint')\n",
        "# parser.add_argument('--exp', default='cifar10_mixup', type=str,\n",
        "#                     help='name of the experiment')\n",
        "# parser.add_argument('--mixup', action='store_true',\n",
        "#                     help='whether to use mixup or not')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# use_cuda = torch.cuda.is_available()\n",
        "# best_acc = 0  # best test accuracy\n",
        "# start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ],
      "metadata": {
        "id": "Rq0RV4vSqTN1"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint_{}/ckpt.t7'.format(args.exp))\n",
        "#     net = checkpoint['net']\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "# else:\n",
        "#     print('==> Building model..')\n",
        "#     # net = VGG('VGG19')\n",
        "#     # net = ResNet18()\n",
        "#     #net = PreActResNet18()\n",
        "#     # net = GoogLeNet()\n",
        "#     net = densenet121()\n",
        "#     # net = ResNeXt29_2x64d()\n",
        "#     # net = MobileNet()\n",
        "#     # net = DPN92()\n",
        "#     # net = ShuffleNetG2()\n",
        "#     # net = SENet18()\n",
        "\n",
        "# # if use_cuda:\n",
        "# #     net.cuda()\n",
        "# #     net = torch.nn.DataParallel(\n",
        "# #         net, device_ids=range(torch.cuda.device_count()))\n",
        "# #     cudnn.benchmark = True\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
        "#                       momentum=0.9, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "PX_2TCTgpFDH"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training\n",
        "\n",
        "\n",
        "\n",
        "# def train(epoch):\n",
        "#     \"\"\"Training function.\"\"\"\n",
        "#     print('\\nEpoch: %d' % epoch)\n",
        "#     net.train()\n",
        "#     train_loss = 0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for batch_idx, (inputs, targets) in enumerate(train_dl):\n",
        "#         inputs_shuffle, targets_shuffle = shuffle_minibatch(\n",
        "#             inputs, targets, args.mixup)\n",
        "\n",
        "#         if use_cuda:\n",
        "#             inputs_shuffle, targets_shuffle = inputs_shuffle.cuda(), \\\n",
        "#                 targets_shuffle.cuda()\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         inputs_shuffle, targets_shuffle = Variable(\n",
        "#             inputs_shuffle), Variable(targets_shuffle)\n",
        "\n",
        "#         outputs = net(inputs_shuffle)\n",
        "#         m = nn.LogSoftmax()\n",
        "\n",
        "#         loss = -m(outputs) * targets_shuffle\n",
        "#         loss = torch.sum(loss) / 128\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         train_loss += loss.data[0]\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += targets.size(0)\n",
        "#         _, targets = torch.max(targets_shuffle.data, 1)\n",
        "#         correct += predicted.eq(targets).cpu().sum()\n",
        "\n",
        "#         progress_bar(batch_idx, len(train_dl), 'Epoch %d, Training Loss: %.3f | Acc: %.3f%% (%d/%d)'  # noqa\n",
        "#                      % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))  # noqa\n",
        "\n"
      ],
      "metadata": {
        "id": "n2K3u-Cdn6SV"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test(epoch):\n",
        "#     \"\"\"Testing function.\"\"\"\n",
        "#     global best_acc\n",
        "#     net.eval()\n",
        "#     test_loss = 0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#         if use_cuda:\n",
        "#             inputs, targets = inputs.cuda(), targets.cuda()\n",
        "#         inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
        "#         outputs = net(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "\n",
        "#         test_loss += loss.data[0]\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += targets.size(0)\n",
        "#         correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "#         progress_bar(batch_idx, len(testloader), 'Epoch %d, Test Loss: %.3f | Acc: %.3f%% (%d/%d)'  # noqa\n",
        "#                      % (epoch, test_loss / (batch_idx + 1), 100. * correct / total, correct, total))  # noqa\n",
        "\n",
        "#     # Save checkpoint.\n",
        "#     acc = 100. * correct / total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.module if use_cuda else net,\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint_{}'.format(args.exp)):\n",
        "#             os.mkdir('checkpoint_{}'.format(args.exp))\n",
        "#         torch.save(state, './checkpoint_{}/ckpt.t7'.format(args.exp))\n",
        "#         best_acc = acc\n",
        "\n",
        "# scheduler = lr_scheduler.MultiStepLR(\n",
        "#     optimizer, milestones=[100, 150], gamma=0.1)\n",
        "# for epoch in range(start_epoch, start_epoch + 200):\n",
        "#     scheduler.step()\n",
        "#     train(epoch)\n",
        "#     test(epoch)"
      ],
      "metadata": {
        "id": "BsE_em60oFbq"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = densenet121()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISkRiC41oJfB",
        "outputId": "9d7f09b4-b921-4ea6-f6f4-99d857995512"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (features): Sequential(\n",
              "    (conv0): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (norm0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu0): ReLU(inplace=True)\n",
              "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (denseblock1): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition1): _Transition(\n",
              "      (norm): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(224, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock2): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(112, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(144, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(176, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(208, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(240, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(272, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(304, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(336, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(368, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(368, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(400, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(432, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(464, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(464, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition2): _Transition(\n",
              "      (norm): BatchNorm2d(496, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(496, 248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock3): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer13): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer14): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(664, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer15): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(696, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(696, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer16): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(728, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer17): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(760, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(760, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer18): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(792, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer19): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(824, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer20): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(856, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer21): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(888, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(888, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer22): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(920, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer23): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(952, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(952, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer24): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(984, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(984, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (transition3): _Transition(\n",
              "      (norm): BatchNorm2d(1016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(1016, 508, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "    )\n",
              "    (denseblock4): _DenseBlock(\n",
              "      (denselayer1): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(508, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(508, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer2): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(540, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(540, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer3): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(572, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(572, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer4): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(604, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(604, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer5): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(636, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(636, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer6): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(668, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(668, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer7): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(700, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(700, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer8): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(732, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(732, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer9): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(764, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(764, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer10): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(796, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(796, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer11): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(828, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(828, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer12): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(860, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(860, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer13): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(892, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(892, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer14): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(924, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(924, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer15): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(956, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(956, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (denselayer16): _DenseLayer(\n",
              "        (norm1): BatchNorm2d(988, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(988, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (norm5): BatchNorm2d(1020, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=1020, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# batch_size=128"
      ],
      "metadata": {
        "id": "lJ_HM2UaE4s_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "# val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
        "     "
      ],
      "metadata": {
        "id": "KimBk32RE9eU"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for images, labels in train_dl:\n",
        "#     print('images.shape:', images.shape)"
      ],
      "metadata": {
        "id": "jo6G_fP1F3CC"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_dl:\n",
        "    print('images.shape:', images.shape)\n",
        "    out = model(images)\n",
        "    print('out.shape:', out.shape)\n",
        "    print('out[0]:', out[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr44c6j9sKqW",
        "outputId": "19e1b1cc-ee0e-4a37-e69c-859e6ae7f008"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images.shape: torch.Size([64, 3, 32, 32])\n",
            "out.shape: torch.Size([64, 10])\n",
            "out[0]: tensor([-0.1468, -0.0600,  0.4117,  0.0300,  0.3219, -0.1667,  0.0489, -0.7326,\n",
            "         0.2394,  0.0811], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train_dl = DeviceDataLoader(train_dl, device)\n",
        "# val_dl = DeviceDataLoader(val_dl, device)\n",
        "# to_device(model, device);"
      ],
      "metadata": {
        "id": "eJpjxX_TtQY1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @torch.no_grad()\n",
        "# def evaluate(model, val_loader):\n",
        "#     model.eval()\n",
        "#     outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "#     return model.validation_epoch_end(outputs)\n",
        "\n",
        "# def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "#     history = []\n",
        "#     optimizer = opt_func(model.parameters(), lr)\n",
        "#     for epoch in range(epochs):\n",
        "#         # Training Phase \n",
        "#         model.train()\n",
        "#         train_losses = []\n",
        "#         for batch in train_loader:\n",
        "#             loss = model.training_step(batch)\n",
        "#             train_losses.append(loss)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             optimizer.zero_grad()\n",
        "#         # Validation phase\n",
        "#         result = evaluate(model, val_loader)\n",
        "#         result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "#         model.epoch_end(epoch, result)\n",
        "#         history.append(result)\n",
        "#     return history"
      ],
      "metadata": {
        "id": "yfMpDIsMHXZ7"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = to_device(model(), device)"
      ],
      "metadata": {
        "id": "OHyb-iLVHeuG"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate(model, val_dl)"
      ],
      "metadata": {
        "id": "m1LAKuFrHhLz"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for param in model.parameters() :\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# in_features = model.fc.in_features\n",
        "# last_layer = nn.Linear(in_features, 6)\n",
        "# model.fc = last_layer\n",
        "# model.to(device)\n",
        "     "
      ],
      "metadata": {
        "id": "cMkn8wydIJAR"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import tarfile\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "xYjEYZ6VQehk"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.003)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "vfgC9WDpKAHs"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_total_steps = len(train_dl)\n",
        "print(n_total_steps)\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "  for i, (images, labels) in enumerate(train_dl):\n",
        "# origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "# input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "# Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "# Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (i+1) % 110 == 0:\n",
        "      print(str(epoch+1) + \" epoch loss is\" + str(loss.item()))\n",
        "      # clear_output()\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8H8K0GuKLO4",
        "outputId": "cc830bbe-580d-4f0b-c0c4-5f931cbdacc8"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "704\n",
            "1 epoch loss is2.1458518505096436\n",
            "1 epoch loss is2.064699649810791\n",
            "1 epoch loss is2.0504703521728516\n",
            "1 epoch loss is1.935514211654663\n",
            "1 epoch loss is1.809253454208374\n",
            "1 epoch loss is1.7091575860977173\n",
            "2 epoch loss is1.8020565509796143\n",
            "2 epoch loss is1.69203519821167\n",
            "2 epoch loss is1.6143592596054077\n",
            "2 epoch loss is1.6073991060256958\n",
            "2 epoch loss is1.477949857711792\n",
            "2 epoch loss is1.5207418203353882\n",
            "3 epoch loss is1.5203948020935059\n",
            "3 epoch loss is1.552245020866394\n",
            "3 epoch loss is1.380632996559143\n",
            "3 epoch loss is1.5003529787063599\n",
            "3 epoch loss is1.5014938116073608\n",
            "3 epoch loss is1.6274837255477905\n",
            "4 epoch loss is1.4259432554244995\n",
            "4 epoch loss is1.4933220148086548\n",
            "4 epoch loss is1.3809870481491089\n",
            "4 epoch loss is1.4440377950668335\n",
            "4 epoch loss is1.201269268989563\n",
            "4 epoch loss is1.4288445711135864\n",
            "5 epoch loss is1.0035367012023926\n",
            "5 epoch loss is1.0452858209609985\n",
            "5 epoch loss is1.1777929067611694\n",
            "5 epoch loss is1.3333231210708618\n",
            "5 epoch loss is1.2519408464431763\n",
            "5 epoch loss is1.3290188312530518\n",
            "6 epoch loss is1.1900770664215088\n",
            "6 epoch loss is1.0896949768066406\n",
            "6 epoch loss is1.556962251663208\n",
            "6 epoch loss is1.2795501947402954\n",
            "6 epoch loss is1.6071748733520508\n",
            "6 epoch loss is1.2982659339904785\n",
            "7 epoch loss is1.0394923686981201\n",
            "7 epoch loss is1.4467140436172485\n",
            "7 epoch loss is1.2726058959960938\n",
            "7 epoch loss is1.0555568933486938\n",
            "7 epoch loss is1.2274575233459473\n",
            "7 epoch loss is1.2308056354522705\n",
            "8 epoch loss is0.8529560565948486\n",
            "8 epoch loss is1.142735242843628\n",
            "8 epoch loss is1.1131772994995117\n",
            "8 epoch loss is0.9660332798957825\n",
            "8 epoch loss is1.2604472637176514\n",
            "8 epoch loss is1.5343736410140991\n",
            "9 epoch loss is1.1727609634399414\n",
            "9 epoch loss is1.1958088874816895\n",
            "9 epoch loss is0.9718319773674011\n",
            "9 epoch loss is1.0784785747528076\n",
            "9 epoch loss is1.234545111656189\n",
            "9 epoch loss is1.050011396408081\n",
            "10 epoch loss is0.973239541053772\n",
            "10 epoch loss is0.9676601886749268\n",
            "10 epoch loss is1.1039681434631348\n",
            "10 epoch loss is0.9445376396179199\n",
            "10 epoch loss is1.0106019973754883\n",
            "10 epoch loss is1.3897144794464111\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './densenet.pth'\n",
        "torch.save(model.state_dict(), PATH)\n"
      ],
      "metadata": {
        "id": "fTZqDqmsT6on"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "  # n_correct = 0\n",
        "  # n_samples = 0\n",
        "  # n_class_correct = [0 for i in range(10)]\n",
        "  # n_class_samples = [0 for i in range(10)]\n",
        "  for images, labels in val_dl:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    # if (i+1) % 500 == 0:\n",
        "    #   print(\" loss is\" + str(loss.item()))\n",
        "      # clear_output()\n",
        "print('Finished Training' + str(loss.item()) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhLHkHJrT__S",
        "outputId": "8b499dd8-100e-49fb-cfaa-ceb4fe225ef3"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training1.445401668548584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(epoch):\n",
        "#     \"\"\"Training function.\"\"\"\n",
        "#     print('\\nEpoch: %d' % epoch)\n",
        "#     model.train()\n",
        "#     train_loss = 0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for batch_idx, (inputs, targets) in enumerate(train):\n",
        "#         inputs_shuffle, targets_shuffle = shuffle_minibatch(\n",
        "#             inputs, targets, args.mixup)\n",
        "\n",
        "#         if use_cuda:\n",
        "#             inputs_shuffle, targets_shuffle = inputs_shuffle.cuda(), \\\n",
        "#                 targets_shuffle.cuda()\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         inputs_shuffle, targets_shuffle = Variable(\n",
        "#             inputs_shuffle), Variable(targets_shuffle)\n",
        "\n",
        "#         outputs = net(inputs_shuffle)\n",
        "#         m = nn.LogSoftmax()\n",
        "\n",
        "#         loss = -m(outputs) * targets_shuffle\n",
        "#         loss = torch.sum(loss) / 128\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         train_loss += loss.data[0]\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += targets.size(0)\n",
        "#         _, targets = torch.max(targets_shuffle.data, 1)\n",
        "#         correct += predicted.eq(targets).cpu().sum()\n",
        "\n",
        "#         progress_bar(batch_idx, len(trainloader), 'Epoch %d, Training Loss: %.3f | Acc: %.3f%% (%d/%d)'  # noqa\n",
        "#                      % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))  # noqa"
      ],
      "metadata": {
        "id": "tbheGc8pKPRg"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1OL6b3gMuyD",
        "outputId": "08e7f985-a245-4f67-a332-d58a4ae2db45"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting models\n",
            "  Downloading models-0.9.3.tar.gz (16 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-mAVyNrM2Xa",
        "outputId": "150f792e-09ea-446f-bfbd-546847fd5a2e"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting models\n",
            "  Using cached models-0.9.3.tar.gz (16 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install models==0.9.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PElfx18XNmwk",
        "outputId": "cce1819c-4d04-44ba-b5e6-f842dab829dd"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting models==0.9.3\n",
            "  Using cached models-0.9.3.tar.gz (16 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install models==0.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cs7vrBwNq7b",
        "outputId": "776d8bb3-b14d-4b4a-afb6-38c7f0230d63"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting models==0.4.0\n",
            "  Using cached models-0.4.0.tar.gz (11 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from __future__ import print_function\n",
        "\n",
        "# import argparse\n",
        "# import os\n",
        "\n",
        "# import numpy\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.optim import lr_scheduler\n",
        "# import torch.backends.cudnn as cudnn\n",
        "\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# from models import PreActResNet18\n",
        "# from utils import progress_bar\n",
        "# from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true',\n",
        "#                     help='resume from checkpoint')\n",
        "# parser.add_argument('--exp', default='cifar10_mixup', type=str,\n",
        "#                     help='name of the experiment')\n",
        "# parser.add_argument('--mixup', action='store_true',\n",
        "#                     help='whether to use mixup or not')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# use_cuda = torch.cuda.is_available()\n",
        "# best_acc = 0  # best test accuracy\n",
        "# start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n"
      ],
      "metadata": {
        "id": "f5Vo8h3dLvHf"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYN3ac8rLv2g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}