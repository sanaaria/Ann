{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPECQ2Q1dW74Rm+xwHyo2VZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaaria/Ann/blob/main/Ann_HW04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AXI70-zelxzS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets,transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "5ZLXfPZQl1K7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "پردازش و ترنسفورم کردن دیتا و دانلود دیتا و دادن  آن به دیتا لودر"
      ],
      "metadata": {
        "id": "iBbKV-HjqK7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([transforms.Resize((32,32)), #resises the image so it can be perfect for our model.\n",
        "transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
        "transforms.RandomRotation(10), #Rotates the image to a specified angel\n",
        "transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angle\n",
        "transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
        "transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
        "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalize all the images\n",
        "])\n",
        "transform = transforms.Compose([transforms.Resize((32,32)),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "training_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train) # Data augmentation is only done on t\n",
        "validation_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=100, shuffle=True) # Batch size of 100 i.e to work with 100 images\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size =100, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVO-NV81l5PZ",
        "outputId": "8a1c67a5-a5aa-46b0-b1cc-451b6020b43a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "train_dataset=images[:1000]\n",
        "data_iter_val = iter(validation_loader)\n",
        "images, labels = next(data_iter_val)\n",
        "valid_dataset=images[:1000]\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=2 ,shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=2,shuffle=True)"
      ],
      "metadata": {
        "id": "mA8JOdb5l8Py"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ساخت تابعی برای میانگین گرفتن از عکس ها"
      ],
      "metadata": {
        "id": "33ySxi9vqbAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean(img1,img2):\n",
        "  return (img1+img2)/2"
      ],
      "metadata": {
        "id": "ljmMI-NPl-o1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "نوشتن کلاس AE"
      ],
      "metadata": {
        "id": "IMDK23neqf2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class AE(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, 4, stride=1, padding=1), #(32 , 29 , 29)\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, 4, stride=1, padding=1), #(64 , 27 , 27)\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(64, 128, 4, stride=1, padding=1))#(128 , 24 , 24)\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(128, 128, 4, stride=2, padding=1)#(128 , 11 , 11)))\n",
        "    self.decoder1 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(128, 64, 3, stride=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.ConvTranspose2d(64, 3, 3, stride=2), # (3,63,63)\n",
        "    nn.UpsamplingNearest2d(scale_factor=32/63)\n",
        "    )\n",
        "    self.decoder2 = nn.Sequential(\n",
        "    nn.ConvTranspose2d(128, 64, 3, stride=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.ConvTranspose2d(64, 3, 3, stride=2), # (3,63,63)\n",
        "    nn.UpsamplingNearest2d(scale_factor=32/63))\n",
        "  def forward(self, x):\n",
        "    \n",
        "      coding = self.encoder(x)\n",
        "      return self.decoder1(coding),self.decoder2(coding)"
      ],
      "metadata": {
        "id": "s-8a-tLtmA3G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AE رو به مدل میدیم و لاس و اپتیمایزر رو تعریف میکنیم"
      ],
      "metadata": {
        "id": "WYPYMcWZqkNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = AE()\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(Model.parameters(),lr=0.0003)"
      ],
      "metadata": {
        "id": "h4M20Ji9mQOT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "مدل رو اموزش میدیم و لاس رو محاسبه میکنیم"
      ],
      "metadata": {
        "id": "MU93UH9MqqJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_total_steps = len(train_loader)\n",
        "print(n_total_steps)\n",
        "Model.train()\n",
        "for epoch in range(10):\n",
        "  for i,images in enumerate(train_loader):\n",
        "      batch_size=images.shape[0]\n",
        "      meanimage = mean(images[:int(batch_size/2)],images[int(batch_size/2):batch_size])\n",
        "      \n",
        "      out1,out2 = Model(meanimage)\n",
        "      loss1 = criterion(out1, images[:int(batch_size/2)])\n",
        "      loss2 = criterion(out2,images[int(batch_size/2):batch_size])\n",
        "      loss=loss1+loss2\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if (i+1) % n_total_steps == 0:\n",
        "        print(str(epoch+1) + \" epoch loss is\" + str(loss.item()))\n",
        "        with torch.no_grad():\n",
        "          for j,images in enumerate(valid_loader):\n",
        "              images = images\n",
        "              batch_size=images.shape[0]\n",
        "              meanimage = mean(images[:int(batch_size/2)],images[int(batch_size/2):batch_size])\n",
        "              out1,out2 = Model(meanimage)\n",
        "              loss1 = criterion(out1, images[:int(batch_size/2)])\n",
        "              loss2 = criterion(out2,images[int(batch_size/2):batch_size])\n",
        "              loss=loss1+loss2\n",
        "              if (j+1) % n_total_steps == 0:\n",
        "                    print(str(epoch+1 ) + \" epoch loss in valid set is\" + str(loss.item()))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJoRSqtQnC-8",
        "outputId": "8629fed7-de53-4ba9-dca0-510c6d6ce6df"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "1 epoch loss is1.0035861730575562\n",
            "1 epoch loss in valid set is0.7325537800788879\n",
            "2 epoch loss is0.7326023578643799\n",
            "2 epoch loss in valid set is0.581794261932373\n",
            "3 epoch loss is0.5817581415176392\n",
            "3 epoch loss in valid set is0.7769557237625122\n",
            "4 epoch loss is0.4572448134422302\n",
            "4 epoch loss in valid set is0.5315554141998291\n",
            "5 epoch loss is0.5120267868041992\n",
            "5 epoch loss in valid set is0.7711190581321716\n",
            "6 epoch loss is0.58022141456604\n",
            "6 epoch loss in valid set is0.4603534936904907\n",
            "7 epoch loss is0.7877572178840637\n",
            "7 epoch loss in valid set is0.4759162664413452\n",
            "8 epoch loss is0.5571469068527222\n",
            "8 epoch loss in valid set is0.6216781735420227\n",
            "9 epoch loss is0.629176139831543\n",
            "9 epoch loss in valid set is0.6638098955154419\n",
            "10 epoch loss is0.7662513256072998\n",
            "10 epoch loss in valid set is0.5260406732559204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zpYkLKJLnuM0"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}